import pandas as pd
import features_extractor as fe
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import joblib
import numpy as np

def elbow_method(dataset):
    wcss = []
    for i in range(1, 11):
        kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42, n_init='auto')
        kmeans.fit(dataset)
        wcss.append(kmeans.inertia_)
    plt.plot(range(1, 11), wcss)
    plt.title('The Elbow Method')
    plt.xlabel('Number of clusters')
    plt.ylabel('WCSS')
    plt.show()

def train_model(dataset, export_file):
    kmeans = KMeans(n_clusters = 2, init = 'k-means++', random_state = 42, n_init='auto')
    kmeans.fit(dataset)
    joblib.dump(kmeans, export_file)

def calculate_only(dataset, input_file):
    # Load the saved KMeans model from file
    loaded_model = joblib.load(input_file)

    # Use the loaded model to make predictions on new data
    predicted_labels = loaded_model.predict(dataset)

    # Print the predicted cluster labels
    print(predicted_labels)

    return predicted_labels

def search_outliers(dataset, input_file):
    kmeans = joblib.load(input_file)

    # Compute the distance of each data point to its cluster center
    distances = np.min(kmeans.transform(dataset), axis=1)

    # Set a threshold to identify outliers
    threshold = np.percentile(distances, 99)

    # Identify the indices of the outlier data points
    outliers = np.where(distances > threshold)[0]

    # Print the indices of the outlier data points
    print(outliers)

def features_reduction(dataset):
    # Create PCA object to reduce features to 3
    pca = PCA(n_components=3)

    # Fit and transform the data to the new feature space
    X_reduced = pca.fit_transform(dataset)

    return X_reduced

def plot_clusters(dataset, y_kmeans):
    fig = plt.figure(figsize=(12, 12))
    ax = fig.add_subplot(projection='3d')
    ax.scatter(dataset[y_kmeans == 0, 0], dataset[y_kmeans == 0, 1], dataset[y_kmeans == 0, 2], s = 100, c = 'red', label = 'Cluster 1')
    ax.scatter(dataset[y_kmeans == 1, 0], dataset[y_kmeans == 1, 1], dataset[y_kmeans == 1, 2], s = 100, c = 'blue', label = 'Cluster 2')
    #plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 300, c = 'yellow', label = 'Centroids')
    plt.title('Clusters')
    plt.legend()
    plt.show()
    pass

def test():
    from sklearn.datasets import make_blobs
    from sklearn.cluster import KMeans

    # Generate some random data with 3 clusters
    X, y = make_blobs(n_samples=100, centers=3, random_state=42)

    # Create a KMeans clustering model
    kmeans = KMeans(n_clusters=3, random_state=42)

    # Fit the model and predict the cluster labels using fit_predict
    labels_fit_predict = kmeans.fit_predict(X)

    # Predict the cluster labels using predict after fitting the model
    kmeans.fit(X)
    labels_predict = kmeans.predict(X)

    # Compare the two label arrays
    print(labels_fit_predict)
    print(labels_predict)
    print(np.array_equal(labels_fit_predict, labels_predict))

if __name__ == "__main__":
    #test()
    dataset = fe.main().values
    joblib_file = 'kmeans_model.joblib'
    dataset = features_reduction(dataset)
    train_model(dataset, joblib_file)
    y_kmeans = calculate_only(dataset, joblib_file)
    search_outliers(dataset, joblib_file)
    plot_clusters(dataset, y_kmeans)